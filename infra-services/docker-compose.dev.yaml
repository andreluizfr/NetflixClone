version: "3"

services:
  #In case of deserialization error
  #docker exec -it redis redis-cli -a netflixredis6379 FLUSHALL
  redis:
    container_name: redis-container
    hostname: redis
    image: redis:7.2
    command: redis-server --requirepass netflixredis6379
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 100
    deploy:
      resources:
        limits:
          cpus: '0.08'
          memory: 128M
        reservations:
          cpus: '0.02'
          memory: 32M
  kafka:
    image: 'bitnami/kafka:3.3.1'
    container_name: kafka-container
    ports:
      - 9092:9092
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_KRAFT_CLUSTER_ID=r4zt_wrqTRuT7W2NJsB_GA
    volumes:
      - .kafkadata:/bitnami/kafka
    healthcheck:
      test: "kafka-cluster.sh cluster-id --bootstrap-server localhost:9092 || exit 1"
      timeout: 5s
      interval: 10s
      retries: 1000
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 5
        window: 240s

  # kafka-without-kraft-mode:
  #   image: confluentinc/confluent-local:7.4.1
  #   hostname: kafka
  #   container_name: kafka-container
  #   ports:
  #     - 8082:8082
  #     - 9092:9092
  #     - 9101:9101
  #   environment:
  #     KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
  #     KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
  #     KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'

    # kafka-connect:
  #   image: confluentinc/cp-kafka-connect:7.2.1
  #   ports:
  #     - 8083:8083
  #   depends_on:
  #     - kafka
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: kafka0:29092
  #     CONNECT_GROUP_ID: compose-connect-group
  #     CONNECT_CONFIG_STORAGE_TOPIC: _connect_configs
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_OFFSET_STORAGE_TOPIC: _connect_offset
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_STATUS_STORAGE_TOPIC: _connect_status
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  #     CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry0:8085
  #     CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry0:8085
  #     CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #     CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #     CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect0
  #     CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components,/usr/share/filestream-connectors,/tmp/kfk"
  #   volumes:
  #     - /tmp/kfk:/tmp/kfk:ro
  #     - /tmp/kfk/test.txt:/tmp/kfk/test.txt
  
  kafka-ui:
    container_name: kafka-ui
    image: 'provectuslabs/kafka-ui:latest'
    ports:
      - 9999:8080
    environment:
      - KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_NAME=r4zt_wrqTRuT7W2NJsB_GA
    depends_on:
      kafka:
        condition: service_healthy
    restart: always

  mongo:
    hostname: mongo
    container_name: mongo-container
    image: mongo
    ports:
      - 27017:27017
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: mongo
      MONGO_INITDB_ROOT_PASSWORD: password

  # elastic-search:
  #   container_name: elastic-search-container
  #   hostname: elastic-search
  #   depends_on:
  #     setup:
  #       condition: service_healthy
  #   image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  #   labels:
  #     co.elastic.logs/module: elasticsearch
  #   volumes:
  #     - certs:/usr/share/elasticsearch/config/certs
  #     - esdata:/usr/share/elasticsearch/data
  #   ports:
  #     - 9200:9200
  #   environment:
  #     - node.name=es01
  #     - cluster.name=${CLUSTER_NAME}
  #     - discovery.type=single-node
  #     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
  #     - bootstrap.memory_lock=true
  #     - xpack.security.enabled=true
  #     - xpack.security.http.ssl.enabled=true
  #     - xpack.security.http.ssl.key=certs/es01/es01.key
  #     - xpack.security.http.ssl.certificate=certs/es01/es01.crt
  #     - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
  #     - xpack.security.transport.ssl.enabled=true
  #     - xpack.security.transport.ssl.key=certs/es01/es01.key
  #     - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
  #     - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
  #     - xpack.security.transport.ssl.verification_mode=certificate
  #     - xpack.license.self_generated.type=${LICENSE}
  #   mem_limit: ${ES_MEM_LIMIT}
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   healthcheck:
  #     test:
  #       [
  #         "CMD-SHELL",
  #         "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
  #       ]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 120